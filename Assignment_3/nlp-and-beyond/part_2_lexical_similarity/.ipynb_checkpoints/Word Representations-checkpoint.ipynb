{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Word Representations and Lexical Similarities\n",
    "\n",
    "This part has 20 points in total.\n",
    "\n",
    "Here we will compare different measures of semantic similarity between words: (1) WordNet depth distance (2) and cosine similarity of words using a given GloVe model features.\n",
    "\n",
    "For more reading on vector semantics got to Chapter 6, sections 6.4 and 6.8:\n",
    "https://web.stanford.edu/~jurafsky/slp3/6.pdf\n",
    "\n",
    "To learn about Wordnet: https://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "For additional Wordnet discussions see Chapter 19: https://web.stanford.edu/~jurafsky/slp3/19.pdf\n",
    "\n",
    "The GloVe word embeddings are described in [this paper](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "## Part 2.1: Semantic similarity with WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# load word-vector glov\n",
    "import gensim.downloader as gensim_api\n",
    "glove_model = gensim_api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "from itertools import combinations, product\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_words = ['car', 'dog', 'banana', 'delicious', 'baguette', 'jumping', 'hugging', 'election']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Word Representations in English WordNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each word above print their synsets\n",
    "# for each synset print all lemmas, hypernyms, hyponyms\n",
    "\n",
    "# \n",
    "# Write your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure The Lexical Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wu-Palmer Similarity is a measure of similarity between to sense based on their depth distance. \n",
    "#\n",
    "# For each pair of words, find their closes sense based on Wu-Palmer Similarity.\n",
    "# List all word pairs and their highest possible wup_similarity. \n",
    "# Use wn.wup_similarity(s1, s2) and itertools (combinations and product).\n",
    "# if there is no connection between two words, put 0.\n",
    "\n",
    "wn_sims = []\n",
    "for word1, word2 in combinations(some_words, 2):\n",
    "    ### Your code here ###\n",
    "    max_sim = 0\n",
    "    ######################\n",
    "    wn_sims.append(max_sim)\n",
    "    print(f\"{word1:9} {word2:9} {max_sim:6.3f}\")\n",
    "\n",
    "# which word pair are the most similar words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2: Semantic similarity with GloVe and comparison with WordNet\n",
    "\n",
    "### Measure the similarities on GloVe Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glov_sims = []\n",
    "for word1, word2 in combinations(some_words, 2):\n",
    "    max_sim = glove_model.similarity(word1, word2)\n",
    "    glov_sims.append(max_sim)\n",
    "    print(f\"{word1:9} {word2:9} {max_sim:6.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine if two measures correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a correlation coefficent of two lists\n",
    "print(\"Spearman's rho\", spearmanr(glov_sims, wn_sims))\n",
    "\n",
    "# Higher correlation (closer to 1.0) means two measures agree with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the two similarities compare? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vector Representations in GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each word is represented as a vector:\n",
    "print('dog =', glove_model['dog'])\n",
    "\n",
    "# matrix of all word vectors is trained as parameters of a language model:\n",
    "# P( target_word | context_word ) = f(word, context ; params)\n",
    "#\n",
    "# Words in a same sentence and in close proximity are in context of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Cosine Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on equation 6.10 J&M (2019)\n",
    "# https://web.stanford.edu/~jurafsky/slp3/6.pdf\n",
    "#\n",
    "def cosine_sim(v1, v2):\n",
    "    out = 0\n",
    "    # code here\n",
    "    return out\n",
    "\n",
    "\n",
    "cosine_sim(glove_model['car'], glove_model['automobile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement top-n most similar words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search in glove_model:\n",
    "def top_n(word, n):\n",
    "    # example: top_n('dog', 3) =  \n",
    "    #[('cat', 0.9218005537986755),\n",
    "    # ('dogs', 0.8513159155845642),\n",
    "    # ('horse', 0.7907583713531494)]\n",
    "    # similar to glove_model.most_similar('dog', topn=3)\n",
    "\n",
    "    out = []\n",
    "    #\n",
    "    # code here\n",
    "    # \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VG Part: Examine Fairness In Data Driven Word Vectors\n",
    "\n",
    "\n",
    "Caliskan et al. (2017) argues that word vectors learn human biases from data. \n",
    "\n",
    "Try to replicate one of the tests of the paper:\n",
    "\n",
    "Caliskan, Aylin, Joanna J. Bryson, and Arvind Narayanan. “Semantics derived automatically from language corpora contain human-like biases.” Science\n",
    "356.6334 (2017): 183-186. http://opus.bath.ac.uk/55288/\n",
    "\n",
    "\n",
    "For example on gender bias:\n",
    "- Male names: John, Paul, Mike, Kevin, Steve, Greg, Jeff, Bill.\n",
    "- Female names: Amy, Joan, Lisa, Sarah, Diana, Kate, Ann, Donna.\n",
    "- Career words : executive, management, professional, corporation, salary, office, business, career.\n",
    "- Family words : home, parents, children, family, cousins, marriage, wedding, relatives.\n",
    "\n",
    "\n",
    "Report the average cosine similarity of male names to career words, and compare it with the average similarity of female names to career words. (repeat for family words) \n",
    "\n",
    "tokens in GloVe model are all in lower case.\n",
    "\n",
    "Write at least one sentence to describe your observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
