{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Word Representations and Lexical Similarities\n",
    "\n",
    "This part has 20 points in total.\n",
    "\n",
    "Here we will compare different measures of semantic similarity between words: (1) WordNet depth distance (2) and cosine similarity of words using a given GloVe model features.\n",
    "\n",
    "For more reading on vector semantics got to Chapter 6, sections 6.4 and 6.8:\n",
    "https://web.stanford.edu/~jurafsky/slp3/6.pdf\n",
    "\n",
    "To learn about Wordnet: https://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "For additional Wordnet discussions see Chapter 19: https://web.stanford.edu/~jurafsky/slp3/19.pdf\n",
    "\n",
    "The GloVe word embeddings are described in [this paper](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "## Part 2.1: Semantic similarity with WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wordnet\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# load word-vector glov\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "glove_model = gensim_api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "\n",
    "from itertools import combinations, product\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_words = ['car', 'dog', 'banana', 'delicious', 'baguette', 'jumping', 'hugging', 'election']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Word Representations in English WordNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')], [Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')], [Synset('banana.n.01'), Synset('banana.n.02')], [Synset('delicious.n.01'), Synset('delightful.s.01'), Synset('delectable.s.01')], [Synset('baguet.n.01')], [Synset('jumping.n.01'), Synset('jump.n.06'), Synset('jump.v.01'), Synset('startle.v.02'), Synset('jump.v.03'), Synset('jump.v.04'), Synset('leap_out.v.01'), Synset('jump.v.06'), Synset('rise.v.11'), Synset('jump.v.08'), Synset('derail.v.02'), Synset('chute.v.01'), Synset('jump.v.11'), Synset('jumpstart.v.01'), Synset('jump.v.13'), Synset('leap.v.02'), Synset('alternate.v.01')], [Synset('caressing.n.01'), Synset('embrace.v.02'), Synset('hug.v.02')], [Synset('election.n.01'), Synset('election.n.02'), Synset('election.n.03'), Synset('election.n.04')]]\n",
      "[[Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'), Lemma('car.n.01.machine'), Lemma('car.n.01.motorcar')], [Lemma('car.n.02.car'), Lemma('car.n.02.railcar'), Lemma('car.n.02.railway_car'), Lemma('car.n.02.railroad_car')], [Lemma('car.n.03.car'), Lemma('car.n.03.gondola')], [Lemma('car.n.04.car'), Lemma('car.n.04.elevator_car')], [Lemma('cable_car.n.01.cable_car'), Lemma('cable_car.n.01.car')], [Lemma('dog.n.01.dog'), Lemma('dog.n.01.domestic_dog'), Lemma('dog.n.01.Canis_familiaris')], [Lemma('frump.n.01.frump'), Lemma('frump.n.01.dog')], [Lemma('dog.n.03.dog')], [Lemma('cad.n.01.cad'), Lemma('cad.n.01.bounder'), Lemma('cad.n.01.blackguard'), Lemma('cad.n.01.dog'), Lemma('cad.n.01.hound'), Lemma('cad.n.01.heel')], [Lemma('frank.n.02.frank'), Lemma('frank.n.02.frankfurter'), Lemma('frank.n.02.hotdog'), Lemma('frank.n.02.hot_dog'), Lemma('frank.n.02.dog'), Lemma('frank.n.02.wiener'), Lemma('frank.n.02.wienerwurst'), Lemma('frank.n.02.weenie')], [Lemma('pawl.n.01.pawl'), Lemma('pawl.n.01.detent'), Lemma('pawl.n.01.click'), Lemma('pawl.n.01.dog')], [Lemma('andiron.n.01.andiron'), Lemma('andiron.n.01.firedog'), Lemma('andiron.n.01.dog'), Lemma('andiron.n.01.dog-iron')], [Lemma('chase.v.01.chase'), Lemma('chase.v.01.chase_after'), Lemma('chase.v.01.trail'), Lemma('chase.v.01.tail'), Lemma('chase.v.01.tag'), Lemma('chase.v.01.give_chase'), Lemma('chase.v.01.dog'), Lemma('chase.v.01.go_after'), Lemma('chase.v.01.track')], [Lemma('banana.n.01.banana'), Lemma('banana.n.01.banana_tree')], [Lemma('banana.n.02.banana')], [Lemma('delicious.n.01.Delicious')], [Lemma('delightful.s.01.delightful'), Lemma('delightful.s.01.delicious')], [Lemma('delectable.s.01.delectable'), Lemma('delectable.s.01.delicious'), Lemma('delectable.s.01.luscious'), Lemma('delectable.s.01.pleasant-tasting'), Lemma('delectable.s.01.scrumptious'), Lemma('delectable.s.01.toothsome'), Lemma('delectable.s.01.yummy')], [Lemma('baguet.n.01.baguet'), Lemma('baguet.n.01.baguette')], [Lemma('jumping.n.01.jumping')], [Lemma('jump.n.06.jump'), Lemma('jump.n.06.jumping')], [Lemma('jump.v.01.jump'), Lemma('jump.v.01.leap'), Lemma('jump.v.01.bound'), Lemma('jump.v.01.spring')], [Lemma('startle.v.02.startle'), Lemma('startle.v.02.jump'), Lemma('startle.v.02.start')], [Lemma('jump.v.03.jump')], [Lemma('jump.v.04.jump')], [Lemma('leap_out.v.01.leap_out'), Lemma('leap_out.v.01.jump_out'), Lemma('leap_out.v.01.jump'), Lemma('leap_out.v.01.stand_out'), Lemma('leap_out.v.01.stick_out')], [Lemma('jump.v.06.jump')], [Lemma('rise.v.11.rise'), Lemma('rise.v.11.jump'), Lemma('rise.v.11.climb_up')], [Lemma('jump.v.08.jump'), Lemma('jump.v.08.leap'), Lemma('jump.v.08.jump_off')], [Lemma('derail.v.02.derail'), Lemma('derail.v.02.jump')], [Lemma('chute.v.01.chute'), Lemma('chute.v.01.parachute'), Lemma('chute.v.01.jump')], [Lemma('jump.v.11.jump'), Lemma('jump.v.11.leap')], [Lemma('jumpstart.v.01.jumpstart'), Lemma('jumpstart.v.01.jump-start'), Lemma('jumpstart.v.01.jump')], [Lemma('jump.v.13.jump'), Lemma('jump.v.13.pass_over'), Lemma('jump.v.13.skip'), Lemma('jump.v.13.skip_over')], [Lemma('leap.v.02.leap'), Lemma('leap.v.02.jump')], [Lemma('alternate.v.01.alternate'), Lemma('alternate.v.01.jump')], [Lemma('caressing.n.01.caressing'), Lemma('caressing.n.01.cuddling'), Lemma('caressing.n.01.fondling'), Lemma('caressing.n.01.hugging'), Lemma('caressing.n.01.kissing'), Lemma('caressing.n.01.necking'), Lemma('caressing.n.01.petting'), Lemma('caressing.n.01.smooching'), Lemma('caressing.n.01.snuggling')], [Lemma('embrace.v.02.embrace'), Lemma('embrace.v.02.hug'), Lemma('embrace.v.02.bosom'), Lemma('embrace.v.02.squeeze')], [Lemma('hug.v.02.hug')], [Lemma('election.n.01.election')], [Lemma('election.n.02.election')], [Lemma('election.n.03.election')], [Lemma('election.n.04.election')]]\n",
      "[[Synset('motor_vehicle.n.01')], [Synset('wheeled_vehicle.n.01')], [Synset('compartment.n.02')], [Synset('compartment.n.02')], [Synset('compartment.n.02')], [Synset('canine.n.02'), Synset('domestic_animal.n.01')], [Synset('unpleasant_woman.n.01')], [Synset('chap.n.01')], [Synset('villain.n.01')], [Synset('sausage.n.01')], [Synset('catch.n.06')], [Synset('support.n.10')], [Synset('pursue.v.02')], [Synset('herb.n.01')], [Synset('edible_fruit.n.01')], [Synset('eating_apple.n.01')], [], [], [Synset('french_bread.n.01')], [Synset('track_and_field.n.01')], [Synset('propulsion.n.02')], [Synset('move.v.03')], [Synset('move.v.03')], [Synset('assail.v.01')], [Synset('wax.v.02')], [Synset('look.v.02')], [Synset('enter.v.02')], [Synset('change.v.02')], [Synset('move.v.03')], [Synset('travel.v.01')], [Synset('dive.v.01')], [], [Synset('start.v.08')], [Synset('neglect.v.01')], [Synset('switch.v.03')], [Synset('change.v.03')], [Synset('foreplay.n.01')], [Synset('clasp.v.01')], [Synset('touch.v.05')], [Synset('vote.n.02')], [Synset('choice.n.02')], [Synset('status.n.01')], [Synset('predestination.n.02')]]\n",
      "[[Synset('ambulance.n.01'), Synset('beach_wagon.n.01'), Synset('bus.n.04'), Synset('cab.n.03'), Synset('compact.n.03'), Synset('convertible.n.01'), Synset('coupe.n.01'), Synset('cruiser.n.01'), Synset('electric.n.01'), Synset('gas_guzzler.n.01'), Synset('hardtop.n.01'), Synset('hatchback.n.01'), Synset('horseless_carriage.n.01'), Synset('hot_rod.n.01'), Synset('jeep.n.01'), Synset('limousine.n.01'), Synset('loaner.n.02'), Synset('minicar.n.01'), Synset('minivan.n.01'), Synset('model_t.n.01'), Synset('pace_car.n.01'), Synset('racer.n.02'), Synset('roadster.n.01'), Synset('sedan.n.01'), Synset('sport_utility.n.01'), Synset('sports_car.n.01'), Synset('stanley_steamer.n.01'), Synset('stock_car.n.01'), Synset('subcompact.n.01'), Synset('touring_car.n.01'), Synset('used-car.n.01')], [Synset('baggage_car.n.01'), Synset('cabin_car.n.01'), Synset('club_car.n.01'), Synset('freight_car.n.01'), Synset('guard's_van.n.01'), Synset('handcar.n.01'), Synset('mail_car.n.01'), Synset('passenger_car.n.01'), Synset('slip_coach.n.01'), Synset('tender.n.04'), Synset('van.n.03')], [], [], [], [Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), Synset('great_pyrenees.n.01'), Synset('griffon.n.02'), Synset('hunting_dog.n.01'), Synset('lapdog.n.01'), Synset('leonberg.n.01'), Synset('mexican_hairless.n.01'), Synset('newfoundland.n.01'), Synset('pooch.n.01'), Synset('poodle.n.01'), Synset('pug.n.01'), Synset('puppy.n.01'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('working_dog.n.01')], [], [], [Synset('perisher.n.01')], [Synset('vienna_sausage.n.01')], [], [], [Synset('hound.v.01'), Synset('quest.v.02'), Synset('run_down.v.07'), Synset('tree.v.03')], [Synset('abaca.n.02'), Synset('dwarf_banana.n.01'), Synset('edible_banana.n.01'), Synset('japanese_banana.n.01'), Synset('plantain.n.02')], [], [Synset('golden_delicious.n.01'), Synset('red_delicious.n.01')], [], [], [], [Synset('broad_jump.n.02'), Synset('high_jump.n.02')], [Synset('capriole.n.01'), Synset('header.n.07'), Synset('hop.n.01'), Synset('jumping_up_and_down.n.01'), Synset('leap.n.01'), Synset('vault.n.04')], [Synset('bounce.v.01'), Synset('bounce.v.05'), Synset('burst.v.04'), Synset('caper.v.01'), Synset('capriole.v.01'), Synset('curvet.v.01'), Synset('galumph.v.01'), Synset('hop.v.01'), Synset('hop.v.06'), Synset('leapfrog.v.01'), Synset('pronk.v.01'), Synset('saltate.v.02'), Synset('ski_jump.v.01'), Synset('vault.v.01'), Synset('vault.v.02')], [Synset('boggle.v.01'), Synset('jackrabbit.v.01'), Synset('rear_back.v.02'), Synset('shy.v.01')], [], [], [], [], [], [], [], [Synset('sky_dive.v.01')], [], [], [], [], [], [Synset('snogging.n.01')], [Synset('clinch.v.04'), Synset('cuddle.v.02'), Synset('interlock.v.03')], [], [Synset('by-election.n.01'), Synset('general_election.n.01'), Synset('primary.n.01'), Synset('reelection.n.01'), Synset('runoff.n.02')], [Synset('co-option.n.01'), Synset('cumulative_vote.n.01')], [], []]\n"
     ]
    }
   ],
   "source": [
    "# For each word above print their synsets\n",
    "# for each synset print all lemmas, hypernyms, hyponyms\n",
    "word_synsets = []\n",
    "for word in some_words:\n",
    "    word_synsets.append(wn.synsets(word))\n",
    "print(word_synsets)\n",
    "\n",
    "lemmas = []\n",
    "hypernyms = []\n",
    "hyponyms = []\n",
    "for word_synset in word_synsets:\n",
    "    for each_synset in word_synset:\n",
    "        lemmas.append(each_synset.lemmas())\n",
    "        hypernyms.append(each_synset.hypernyms())\n",
    "        hyponyms.append(each_synset.hyponyms())\n",
    "print(lemmas)\n",
    "print(hypernyms)\n",
    "print(hyponyms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure The Lexical Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car       dog        0.667\n",
      "car       banana     0.421\n",
      "car       delicious  0.364\n",
      "car       baguette   0.211\n",
      "car       jumping    0.167\n",
      "car       hugging    0.235\n",
      "car       election   0.133\n",
      "dog       banana     0.632\n",
      "dog       delicious  0.556\n",
      "dog       baguette   0.556\n",
      "dog       jumping    0.333\n",
      "dog       hugging    0.286\n",
      "dog       election   0.182\n",
      "banana    delicious  0.750\n",
      "banana    baguette   0.556\n",
      "banana    jumping    0.167\n",
      "banana    hugging    0.250\n",
      "banana    election   0.143\n",
      "delicious baguette   0.500\n",
      "delicious jumping    0.500\n",
      "delicious hugging    0.400\n",
      "delicious election   0.222\n",
      "baguette  jumping    0.154\n",
      "baguette  hugging    0.222\n",
      "baguette  election   0.125\n",
      "jumping   hugging    0.400\n",
      "jumping   election   0.667\n",
      "hugging   election   0.200\n",
      "('banana', 'delicious') 0.75\n"
     ]
    }
   ],
   "source": [
    "# Wu-Palmer Similarity is a measure of similarity between to sense based on their depth distance. \n",
    "#\n",
    "# For each pair of words, find their closes sense based on Wu-Palmer Similarity.\n",
    "# List all word pairs and their highest possible wup_similarity. \n",
    "# Use wn.wup_similarity(s1, s2) and itertools (combinations and product).\n",
    "# if there is no connection between two words, put 0.\n",
    "\n",
    "wn_sims = []\n",
    "for word1, word2 in combinations(some_words, 2):\n",
    "    synsets_w1 = wn.synsets(word1)\n",
    "    synsets_w2 = wn.synsets(word2)\n",
    "    pair = word1 , word2\n",
    "    max_sim = 0\n",
    "    for synset_w1 in synsets_w1:\n",
    "        for synset_w2 in synsets_w2:\n",
    "            similarity = wn.wup_similarity(synset_w1,synset_w2)\n",
    "            if similarity > max_sim:\n",
    "                max_sim = similarity\n",
    "    wn_sims.append(max_sim)\n",
    "      \n",
    "    print(f\"{word1:9} {word2:9} {max_sim:6.3f}\")\n",
    "\n",
    "# which word pair are the most similar words?\n",
    "most_similar_pair = None\n",
    "max_simil = 0\n",
    "\n",
    "for wn_sim, pair in zip(wn_sims, combinations(some_words, 2)):\n",
    "    if wn_sim > max_simil:\n",
    "        most_similar_pair = pair\n",
    "        max_simil = wn_sim\n",
    "\n",
    "\n",
    "print(f\"{most_similar_pair} {max_simil}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2: Semantic similarity with GloVe and comparison with WordNet\n",
    "\n",
    "### Measure the similarities on GloVe Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car       dog        0.464\n",
      "car       banana     0.219\n",
      "car       delicious  0.068\n",
      "car       baguette   0.046\n",
      "car       jumping    0.516\n",
      "car       hugging    0.278\n",
      "car       election   0.333\n",
      "dog       banana     0.333\n",
      "dog       delicious  0.404\n",
      "dog       baguette   0.018\n",
      "dog       jumping    0.539\n",
      "dog       hugging    0.410\n",
      "dog       election   0.181\n",
      "banana    delicious  0.487\n",
      "banana    baguette   0.450\n",
      "banana    jumping    0.108\n",
      "banana    hugging    0.127\n",
      "banana    election   0.164\n",
      "delicious baguette   0.421\n",
      "delicious jumping    0.042\n",
      "delicious hugging    0.142\n",
      "delicious election   0.028\n",
      "baguette  jumping   -0.075\n",
      "baguette  hugging    0.161\n",
      "baguette  election  -0.091\n",
      "jumping   hugging    0.447\n",
      "jumping   election   0.206\n",
      "hugging   election  -0.076\n"
     ]
    }
   ],
   "source": [
    "glov_sims = []\n",
    "for word1, word2 in combinations(some_words, 2):\n",
    "    max_sim = glove_model.similarity(word1, word2)\n",
    "    glov_sims.append(max_sim)\n",
    "    print(f\"{word1:9} {word2:9} {max_sim:6.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine if two measures correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's rho SignificanceResult(statistic=0.4222499442309076, pvalue=0.02519986065189366)\n"
     ]
    }
   ],
   "source": [
    "# a correlation coefficent of two lists\n",
    "print(\"Spearman's rho\", spearmanr(glov_sims, wn_sims))\n",
    "\n",
    "# Higher correlation (closer to 1.0) means two measures agree with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the two similarities compare? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (4156120189.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    The Spearman's rho is approximately 0.422, which suggests a positive correlation between the two lists (glov_sims and wn_sims).\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": [
    " The Spearman's rho is approximately 0.422, which suggests a positive correlation between the two lists (glov_sims and wn_sims). \n",
    "This means that as the values in one list tend to increase, the values in the other list also tend to increase. The p-value associated with the correlation coefficient is 0.0252.\n",
    "Since the p-value is small it means that there is statistically significant correlation and the null hypothesis doesn't apply. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vector Representations in GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog = [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "# Each word is represented as a vector:\n",
    "print('dog =', glove_model['dog'])\n",
    "\n",
    "# matrix of all word vectors is trained as parameters of a language model:\n",
    "# P( target_word | context_word ) = f(word, context ; params)\n",
    "#\n",
    "# Words in a same sentence and in close proximity are in context of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Cosine Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6956218"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# based on equation 6.10 J&M (2019)\n",
    "# https://web.stanford.edu/~jurafsky/slp3/6.pdf\n",
    "#\n",
    "def cosine_sim(v1, v2):\n",
    "    dot_product = np.dot(v1,v2)\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    cos_sim = dot_product / (norm_v1 * norm_v2)\n",
    "    return cos_sim\n",
    "\n",
    "\n",
    "cosine_sim(glove_model['car'], glove_model['automobile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement top-n most similar words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'dogs', 'horse']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search in glove_model:\n",
    "def top_n(word, n):\n",
    "    # example: top_n('dog', 3) =  \n",
    "    #[('cat', 0.9218005537986755),\n",
    "    # ('dogs', 0.8513159155845642),\n",
    "    # ('horse', 0.7907583713531494)]\n",
    "    # similar to glove_model.most_similar('dog', topn=3)\n",
    "    \n",
    "    \n",
    "    word_vec = glove_model[word]\n",
    "    similarities = []\n",
    "    for other_word in glove_model.index_to_key:\n",
    "        if other_word != word:\n",
    "            other_vec = glove_model[other_word]\n",
    "            similarity = cosine_sim(word_vec,other_vec)\n",
    "            similarities.append((other_word,similarity))\n",
    "    sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    top_n_words = []\n",
    "    for other_word, _ in sorted_similarities[:n]:\n",
    "        top_n_words.append(other_word)\n",
    "    \n",
    "    return top_n_words\n",
    "    \n",
    "top_n('dog',3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VG Part: Examine Fairness In Data Driven Word Vectors\n",
    "\n",
    "Caliskan et al. (2017) argues that word vectors learn human biases from data. \n",
    "\n",
    "Try to replicate one of the tests of the paper:\n",
    "\n",
    "Caliskan, Aylin, Joanna J. Bryson, and Arvind Narayanan. “Semantics derived automatically from language corpora contain human-like biases.” Science\n",
    "356.6334 (2017): 183-186. http://opus.bath.ac.uk/55288/\n",
    "\n",
    "\n",
    "For example on gender bias:\n",
    "- Male names: John, Paul, Mike, Kevin, Steve, Greg, Jeff, Bill.\n",
    "- Female names: Amy, Joan, Lisa, Sarah, Diana, Kate, Ann, Donna.\n",
    "- Career words : executive, management, professional, corporation, salary, office, business, career.\n",
    "- Family words : home, parents, children, family, cousins, marriage, wedding, relatives.\n",
    "\n",
    "\n",
    "Report the average cosine similarity of male names to career words, and compare it with the average similarity of female names to career words. (repeat for family words) \n",
    "\n",
    "tokens in GloVe model are all in lower case.\n",
    "\n",
    "Write at least one sentence to describe your observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_names = ['John','Paul','Mike','Kevin','Steve','Greg','Jeff','Bill']\n",
    "female_names = ['Amy','Joan','Lisa','Sarah','Diana','Kate','Ann','Donna']\n",
    "career_words = ['executive','management','professional','corporation','salary','office','business','career']\n",
    "family_words = ['home','parents','children','family','cousins','marriage','wedding','relatives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79418886\n",
      "0.79418886\n"
     ]
    }
   ],
   "source": [
    "cosine_similarities_male_career = []\n",
    "cosine_similarities_male_family = []\n",
    "male_names_edited =[]\n",
    "for name in male_names:\n",
    "    name= name.lower()\n",
    "    male_names_edited.append(name)\n",
    "for name,word in zip(male_names_edited,career_words):\n",
    "        cosine_similarities_male_career.append(cosine_sim(glove_model['name'],glove_model['word']))\n",
    "average_cos_sims_male_career = np.mean(cosine_similarities_male_career)\n",
    "for name,word in zip(male_names_edited,family_words):\n",
    "        cosine_similarities_male_family.append(cosine_sim(glove_model['name'],glove_model['word']))\n",
    "average_cos_sims_male_family = np.mean(cosine_similarities_male_family)\n",
    "print(average_cos_sims_male_career)\n",
    "print(average_cos_sims_male_family)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79418886\n",
      "0.79418886\n"
     ]
    }
   ],
   "source": [
    "cosine_similarities_female_career = []\n",
    "cosine_similarities_female_family = []\n",
    "female_names_edited =[]\n",
    "for name in female_names:\n",
    "    name= name.lower()\n",
    "    female_names_edited.append(name)\n",
    "for name,word in zip(female_names_edited,career_words):\n",
    "        cosine_similarities_female_career.append(cosine_sim(glove_model['name'],glove_model['word']))\n",
    "average_cos_sims_female_career = np.mean(cosine_similarities_female_career)\n",
    "for name,word in zip(female_names_edited,family_words):\n",
    "        cosine_similarities_female_family.append(cosine_sim(glove_model['name'],glove_model['word']))\n",
    "average_cos_sims_female_family = np.mean(cosine_similarities_female_family)\n",
    "print(average_cos_sims_female_career)\n",
    "print(average_cos_sims_female_family)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31931674\n",
      "0.2551345\n"
     ]
    }
   ],
   "source": [
    "glov_sims_male_career = []\n",
    "glov_sims_male_family = []\n",
    "male_names_edited =[]\n",
    "for name in male_names:\n",
    "    name= name.lower()\n",
    "    male_names_edited.append(name)\n",
    "for name, word in zip(male_names_edited, career_words):\n",
    "    max_sim = glove_model.similarity(name, word)\n",
    "    glov_sims_male_career.append(max_sim)\n",
    "average_glov_sim_male_career = np.mean(glov_sims_male_career)\n",
    "for name, word in zip(male_names_edited, family_words):\n",
    "    max_sim = glove_model.similarity(name, word)\n",
    "    glov_sims_male_family.append(max_sim)\n",
    "average_glov_sim_male_family = np.mean(glov_sims_male_family)\n",
    "print(average_glov_sim_male_career)\n",
    "print(average_glov_sim_male_family)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15457344\n",
      "0.34622508\n"
     ]
    }
   ],
   "source": [
    "glov_sims_female_career = []\n",
    "glov_sims_female_family = []\n",
    "female_names_edited =[]\n",
    "for name in female_names:\n",
    "    name = name.lower()\n",
    "    female_names_edited.append(name)\n",
    "for name, word in zip(female_names_edited, career_words):\n",
    "    max_sim = glove_model.similarity(name, word)\n",
    "    glov_sims_female_career.append(max_sim)\n",
    "average_glov_sim_female_career = np.mean(glov_sims_female_career)\n",
    "for name, word in zip(female_names_edited, family_words):\n",
    "    max_sim = glove_model.similarity(name, word)\n",
    "    glov_sims_female_family.append(max_sim)\n",
    "average_glov_sim_female_family = np.mean(glov_sims_female_family)\n",
    "print(average_glov_sim_female_career)\n",
    "print(average_glov_sim_female_family)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I use my cosine similarity function I don't seem to find any difference in the average number of cosine similarities. However, if I use the glove model already existent function there is a significant difference which provides evidence of the corpora gender biases. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
