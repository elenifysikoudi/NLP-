{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Train N-Gram Language Models and answer questions\n",
    "\n",
    "\n",
    "This notebook has 20 points.\n",
    "\n",
    "Here we examine how to build count-based MLE language models.\n",
    "\n",
    "\n",
    "## Part 1.1: Language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "nltk.download('punkt') \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ngram:\n",
    "_N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a wikipedia dataset:\n",
    "#! wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
    "#! unzip wikitext-2-raw-v1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a corpus reader\n",
    "# this includes: sentence segmentation and word tokenization:\n",
    "wikitext2 = PlaintextCorpusReader(\n",
    "    'wikitext-2-raw',\n",
    "    ['wiki.train.raw', 'wiki.valid.raw', 'wiki.test.raw'],\n",
    ")\n",
    "word_tokenizer = wikitext2._word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test split:\n",
    "train = wikitext2.sents('wiki.train.raw')\n",
    "test = wikitext2.sents('wiki.test.raw')\n",
    "\n",
    "# the vocabulary based on the training data:\n",
    "vocab = nltk.lm.Vocabulary([\n",
    "    word\n",
    "    for sent in train\n",
    "    for word in sent\n",
    "], unk_cutoff=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build n-grams\n",
    "def build_ngrams(sent, n):\n",
    "    # pad both ends for corner-ngrams:\n",
    "    sent = ['<s>']*(n-1) + sent + ['</s>']*(n-1)\n",
    "    # build the ngrams:\n",
    "    return list(ngrams(sent, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_tokinized:\n",
      "['Minecraft', 'is', 'a', 'sandbox', 'video', 'game', 'developed', 'by', 'Mojang', '.']\n",
      "sample_trigrams\n",
      "[('<s>', '<s>', 'Minecraft'), ('<s>', 'Minecraft', 'is'), ('Minecraft', 'is', 'a'), ('is', 'a', 'sandbox'), ('a', 'sandbox', 'video'), ('sandbox', 'video', 'game'), ('video', 'game', 'developed'), ('game', 'developed', 'by'), ('developed', 'by', 'Mojang'), ('by', 'Mojang', '.'), ('Mojang', '.', '</s>'), ('.', '</s>', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "# run this cell to inspect how it works:\n",
    "sample = \"Minecraft is a sandbox video game developed by Mojang.\"\n",
    "sample_tokinized = word_tokenizer.tokenize(sample)\n",
    "sample_trigrams = build_ngrams(sample_tokinized, n=3)\n",
    "print('sample_tokinized:')\n",
    "print(sample_tokinized)\n",
    "print('sample_trigrams')\n",
    "print(sample_trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the count model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "plain: 100%|██████████| 3/3 [00:39<00:00, 13.19s/it]\n",
      "smoothing: 100%|██████████| 3/3 [00:39<00:00, 13.15s/it]\n",
      "smoothing+interpolation: 100%|██████████| 3/3 [00:38<00:00, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 56s\n",
      "Wall time: 1min 57s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compare these two models:\n",
    "models = {\n",
    "    'plain': nltk.lm.MLE, # plain count-based ngrams\n",
    "    'smoothing': nltk.lm.Laplace, # with laplace smoothing\n",
    "    'smoothing+interpolation': nltk.lm.KneserNeyInterpolated, # Modified Kneser & Ney \n",
    "}\n",
    "\n",
    "for lm_name in models:\n",
    "    # build and train the language model:\n",
    "    models[lm_name] = models[lm_name](_N, vocabulary=vocab)\n",
    "\n",
    "    # train on all n-grams (equal or lower order): N, N-1, ..., 1.\n",
    "    for n in tqdm(range(_N, 0, -1), desc=lm_name):\n",
    "        models[lm_name].fit([build_ngrams(sent, n) for sent in train])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FreqDist({'the': 113161, ',': 99925, '.': 78888, 'of': 56889, 'and': 50605, 'in': 39488, 'to': 39190, 'a': 34269, '=': 29570, '\"': 28309, ...}),\n",
       " <ConditionalFreqDist with 75988 conditions>,\n",
       " <ConditionalFreqDist with 701600 conditions>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understand how fit words:\n",
    "# fit() method builds all kinds of count dictionaries:\n",
    "(\n",
    "    models['plain'].counts[1], # unigrams\n",
    "    models['plain'].counts[2], # bi-grams for conditional count freq (w_{t} | w_{t-1})\n",
    "    models['plain'].counts[3], # tri-grams for conditional count freq (w_{t} | w_{t-2} w_{t-1})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('A', 'large'),\n",
       " FreqDist({'number': 4, 'variety': 3, 'portion': 3, 'team': 1, 'tent': 1, 'oil': 1, 'pyramid': 1, 'camp': 1, 'rear': 1, 'network': 1, ...}))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example: \n",
    "# Count( word_3='numer'   | word_1 = 'A', word_2 ='large' ) = 4\n",
    "# Count( word_3='variety' | word_1 = 'A', word_2 ='large' ) = 3\n",
    "# ...\n",
    "list(models['plain'].counts[3].items())[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615385"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understand this:\n",
    "models['plain'].counts[3][('A', 'large')]['number'] / sum(models['plain'].counts[3][('A', 'large')].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615385"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['plain'].score('number', ('A', 'large'))\n",
    "# more details in chapter 3 equation 3.12.\n",
    "# https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['began',\n",
       " 'to',\n",
       " 'spread',\n",
       " 'their',\n",
       " 'views',\n",
       " '.',\n",
       " '<UNK>',\n",
       " '<UNK>',\n",
       " 'He',\n",
       " 'refused']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can use the plain model for random language generation:\n",
    "models['plain'].generate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14.527499220336294"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect log probabilities:\n",
    "models['plain'].logscore('mind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Minecraft is a sandbox video game developed by Mojang.\"\n",
    "sample_ngrams = [\n",
    "    None,\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=1), # unigrams\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=2), # bigrams\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=3), # trigrams\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plain model:\n",
      "1-gram inf\n",
      "2-gram inf\n",
      "3-gram inf\n",
      "\n",
      "smoothing model:\n",
      "1-gram 5089.599724571091\n",
      "2-gram 4218.755058726706\n",
      "3-gram 13170.588563844552\n",
      "\n",
      "smoothing+interpolation model:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram 1087.2475507780318\n",
      "2-gram 334.25241384629607\n",
      "3-gram 328.99011711235784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name in models:\n",
    "    print(f\"{model_name} model:\")\n",
    "    for n in range(1, _N+1):\n",
    "        print(f\"{n}-gram\", models[model_name].perplexity(sample_ngrams[n]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why these models have `<UNK>` token? What is the log-probability of `<UNK>` in three models? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The <UNK> token is used to represent unknown tokens in our code that means data that we have not encountered while we were training our model or are very rare words \n",
    "so we defined them as <unk>. It is necessary to have in order to avoid getting a lot of zeros as a result because that would lower the precision of our model.\n",
    "So the <unk> token generalises unseen words and gives predictions. Generalization is good for a model as it learns to handle patterns instead of memorizing specific words.\n",
    "Log-prob of <UNK> in plain model: -inf\n",
    "Log-prob of <UNK> in LaPlace smoothing: -21.03873951979207\n",
    "Log-prob of <UNK> in Interpolation and smoothing: -12.162899323232523 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plain model: -inf\n",
      "\n",
      "smoothing model: -21.03873951979207\n",
      "\n",
      "smoothing+interpolation model: -12.162899323232523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name in models:\n",
    "    print(f\"{model_name} model:\",models[model_name].logscore('<UNK>'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You have already been introduced the concepts of model evaluation, training and test data, probabilities. Explore a related concept of ‘Perplexity’ and answer the following questions. \n",
    "#### See section 3.2.1 from Jurafsky & Martin Chapter 3 for some details on perplexity.\n",
    "#### This might be helpful too. https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Why plain count-based MLE model fails to produce perplexities? What are the possible solutions for it? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "One of the main reasons is the unseen n-grams since their probabilities will be 0 and the perplexity as a result will be -inf. Also, if the vocabulary is too big the model \n",
    "may not have all the necessary information so in the end the probabilities and perplexities it calculates are not trustworthy and if it is too small it cannot generalize.\n",
    "A possible solutions is smoothing techniques as well interpolation and maybe neural language models that however have their own porblems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Show with an example why Laplace smoothing can produce perplexity for unseen words? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"La playa al atardecer es un lugar tranquilo y hermoso.\"\n",
    "sample_ngrams = [\n",
    "    None,\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=1), # unigrams\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=2), # bigrams\n",
    "    build_ngrams(word_tokenizer.tokenize(sample), n=3), # trigrams\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram 162025.13657586955\n",
      "\n",
      "2-gram 29800.809435232753\n",
      "\n",
      "3-gram 25384.451658389768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(1, _N+1):\n",
    "    print(f\"{n}-gram\", models[\"smoothing\"].perplexity(sample_ngrams[n]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "As the example illustrates when using Laplace smoothing the perplexity is produced for unseen words. If we were to use the plain model then the 2-gram and 3-gram\n",
    "perplexity would be -inf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Why perplexity of bi-grams are lower than unigrams? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 75988 conditions>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Perplexity of bigrams is lower because there is more context information whereas unigrams treat each word independently and smoothing makes probabilities bigger.\n",
    "Also, bigrams reduce ambiguity and have a smaller vocabulary and that can lead to a higher probability so lower perplexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VG Part: neural language models and perplexity of conditional trigrams\n",
    "\n",
    "\n",
    "The neural network below is based on Bengio et al. (2003). It is trained on moving windows described in chapter 9 figure 9.1 but with trigrams instead of 4-grams.\n",
    "https://web.stanford.edu/~jurafsky/slp3/9.pdf\n",
    "\n",
    "You don't need to train the model. However, a stand alone python code is provided in `bengio_lm.py` if you want to try training it on GPU.\n",
    "\n",
    "Read the code below then report the perplexity of the language model on the sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;66;03m# neural network framework\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# encoding the tokens:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m vocab_list \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word, freq \u001b[38;5;129;01min\u001b[39;00m vocab\u001b[38;5;241m.\u001b[39mcounts\u001b[38;5;241m.\u001b[39mmost_common() \u001b[38;5;28;01mif\u001b[39;00m freq \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch # neural network framework\n",
    "\n",
    "# encoding the tokens:\n",
    "vocab_list = [word for word, freq in vocab.counts.most_common() if freq > 1]\n",
    "word2idx = {word: idx for idx, word in enumerate(['<s>', '</s>', vocab.unk_label]+vocab_list)}\n",
    "idx2word = {idx: word for idx, word in enumerate(['<s>', '</s>', vocab.unk_label]+vocab_list)}\n",
    "\n",
    "def token_encoder(tokens):\n",
    "    if type(tokens) in {list, tuple}:\n",
    "        return [word2idx[token] if token in word2idx else word2idx[vocab.unk_label] for token in tokens]\n",
    "    elif type(tokens) == str:\n",
    "        token = tokens\n",
    "        return word2idx[token] if token in word2idx else word2idx[vocab.unk_label]\n",
    "    print(type(tokens))\n",
    "\n",
    "# moving window language model:\n",
    "# https://jmlr.org/papers/volume3/tmp/bengio03a.pdf\n",
    "class BengioLM(torch.nn.Module):\n",
    "    def __init__(self, context_size=2, dim=50):\n",
    "        super(BengioLM, self).__init__()\n",
    "        # defining the parameters of the model\n",
    "        self.C = torch.nn.Embedding(len(word2idx), dim) # C\n",
    "        self.Hx_d = torch.nn.Linear(context_size*dim, dim) # d, H\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.Wx_Uf_b = torch.nn.Linear((context_size + 1) * dim, len(word2idx)) # b, U, W\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.loss_fn = torch.nn.NLLLoss() # negative-log-likelihood loss\n",
    "    \n",
    "    def forward(self, context, target_idx=None):\n",
    "        # function of the model\n",
    "        batch_size = context.shape[0]\n",
    "        x = self.C(context).view(batch_size,-1)\n",
    "        x = torch.cat([x, self.tanh(self.Hx_d(x))], dim=-1)\n",
    "        logprob = self.logsoftmax(self.Wx_Uf_b(x))\n",
    "        \n",
    "        if target_idx is None:\n",
    "            return logprob\n",
    "        else:\n",
    "            loss = self.loss_fn(logprob, target_idx)\n",
    "            return logprob, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model is trained with Stochastic Gradient Descent with 10 epochs (skip this):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = BengioLM()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "sentence_batch_size = 32\n",
    "\n",
    "for e in range(10):\n",
    "    progress_bar = tqdm(range(0, len(train), sentence_batch_size))\n",
    "    for i in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        contexts, targets = zip(*[\n",
    "            (token_encoder(tokens[:-1]), token_encoder(tokens[-1]))\n",
    "            for sent in train[i:i+sentence_batch_size]\n",
    "            for tokens in build_ngrams(sent, 3)\n",
    "        ])\n",
    "\n",
    "        logprob, loss = model.forward(torch.tensor(contexts), torch.tensor(targets))\n",
    "        progress_bar.set_description_str(f\"epoch={e+1},loss={loss.item():.3f}\")\n",
    "        progress_bar.update()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# we ran the training code above on GPU and saved it in model.pt.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# load the pre-trained language model:\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m BengioLM()\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# we ran the training code above on GPU and saved it in model.pt.\n",
    "# load the pre-trained language model:\n",
    "device = torch.device('cpu')\n",
    "model = BengioLM()\n",
    "model.load_state_dict(torch.load('model.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how you can get the conditional log-probabilities of all words in the sentence\n",
    "# P(target | w0, w1):\n",
    "for w0, w1, target in build_ngrams(word_tokenizer.tokenize(sample), n=3):\n",
    "    logprobs = model.forward(torch.tensor([token_encoder([w0,w1])]))\n",
    "    print(target, logprobs[0, token_encoder(target)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a code here to report Perplexity of the sample sentence.\n",
    "\n",
    "For more information got to chapter 3, section 3.2.1 and chapter 9, equation 9.12.\n",
    "\n",
    "https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "https://web.stanford.edu/~jurafsky/slp3/9.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity of a sentence \n",
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a generate function using pre-trained language model above\n",
    "\n",
    "For hints see `bengio_lm.py`, for example how the training loop is implemented. The generation loop would be very similar to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
